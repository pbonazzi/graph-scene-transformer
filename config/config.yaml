device: "gpu"

data: 
  n_of_relationships : 44
  n_of_objects: 529
  n_of_orientation_bins: 24  
  n_of_graphs: 1
  batch_size: 64
  enc_dim_object: 23
  enc_dim_rotation_z: 23
  enc_dim_size: 23
  enc_dim_transln: 23

  positional_encoding: laplacian 
  # 'laplacian' | 'wl' | 'none'
  positional_encoding_dim: 4
  positional_encoding_max_graph: 20
  
seed: 41
epochs: 100
n_epochs_stop: 5

encoder:
  floorplan: 'none'
  # 'boxes' | 'scan' | 'none'
  n_of_convolution_layers: 0
  n_of_transformers_layers: 2
  n_of_attention_heads: 8
  type_of_attention: 'graphormer'
  # 'classic' | 'graphormer' | 'dgl_tutorial' | 'as_in_generalization_graph_to_transformer_on_dst' | 'node_edge_combined'
  hidden_dimension: 128
  residual: true
  layer_norm: false
  batch_norm: false

latent: 'prior'
# 'prior' | 'posterior' | 'none'

decoder:
  conditioned_generation: true
  n_of_convolution_layers: 0
  n_of_transformers_layers: 2
  n_of_attention_heads: 8
  type_of_attention: 'graphormer'
  # 'classic' | 'graphormer' | 'dgl_tutorial' | 'as_in_generalization_graph_to_transformer_on_dst' | 'node_edge_combined'
  hidden_dimension: 128
  residual: true
  layer_norm: false
  batch_norm: false

loss: 
  multi_layer: false
  weight_KLD: 0.01
  orientation: true

optimizer: 
  init_lr: 0.00005
  min_lr: 0.0
  weight_decay: 0.0
  betas: [0.9, 0.98]


scheduler: 
  steps: 10
  lr_reduce_factor: 0.99
  lr_schedule_patience: 5

gradient_clipping : true
clipping_value : 5
in_feat_dropout: 0.0
dropout: 0.0

